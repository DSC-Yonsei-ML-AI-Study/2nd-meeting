# 이번 회차 Keyword

#1. 머신러닝 이론
### 회귀와 분류의 공통점과 차이점
분류와 회귀는 모두 지도학습을 목적으로 가지고 있다고 할 수 있다.   
 분류는 미리 정의된, 가능성이 있는 여러 클래스 레이블 중하나를 예측하는 것이다.    
  예를 들어 붓꽃의 품종을 예측하는 것은 분류에 속한다.              
분류는 두 개로 분류하는 
	이진 분류(binary classification)과 셋 이상으로 분류하는 다중 분류(multiclass classification)으로 나누어 진다.    
  	이진 분류는 예 / 아니요만 나올 수 있다고 보면 된다. 
	
예를 들어 남자, 여자로 나눌 수도 있지만, 남자인가? 라는 질문에는 예와 아니요로 바꿀 수 있기 때문에 결국 예 / 아니요라고 볼 수 있다.
	붓꽃 예제의 경우 3개의 클래스를 가지고 있기 때문에 다중 분류이다.

회귀는 연속적인 숫자(실수)를 예측하는 것이다. 어떤 사람의 교육 수준, 나이 등을 이용해 연봉을 예측하는 것도 회귀 문제의 예이고, 
	몸무게를 이용해 키를 예측하는 것도 회귀 문제라고 볼 수 있다.
	출력 값에 연속성이 있다면 회귀 문제라고 볼 수 있다. 연봉을 예상할 때 1억이든 1억 1만원이든 큰 문제가 되지 않는다. 
	하지만 분류 문제에서는 중간은 없다. 
	예를 들어 스팸메일을 분류한다면 스팸 메일이거나 아니거나 두 가지로 나누어지는 것이지 중간인 메일은 없다.
	
회귀는 출력에 연속성이 있고 그 연속성 중에 점을 어디에 찍을지 결정하는 문제이다.
	확률은 사건들의 연속,독립,반복 등의 시행(trial)에 따라 
	표본공간(sample space, 일어날 수 있는 모든 경우의 수)속에서 사건(event)이 발생할 경우의 수를 구하는 문제이다.
![](https://i.imgur.com/EKUPD6V.png)



### 선형 회귀란?

선형회귀는 직선, 즉 일차함수의 개념은 y=ax+b 직선을 임의로 그려놓고, 그 직선을 바탕으로 예측하는 것이다.               
	예측하기 위해 만든 모델인 y=ax+b직선과 실제 데이터를 찍어놓은 점들의 y값 차이를 error라고 한다.	                                                                     
즉, 점과 직선사이의 수직거리가 있어야 'error가 있다'라고 말할 수 있는 것이다.

![](https://i.imgur.com/hXO4geP.png)

	Error- 실제 데이터의 y값과 예측 직선모델의 y값의 차이
	Square error : 실제 데이터의 y값과 예측 직선모델의 
				   y값의 차이를 제곱해서 넓이로 보는 것이다.
	
####Error를 제곱해서 넓이로 보는 이유는 무엇일까?

![](https://i.imgur.com/ijWFTQO.png)		
	
	1. 우리눈에 보이기 쉽다.
	2. 수학적으로 볼 때, 에러가 조금이라도 있다면, 값이 증폭되어 큰값과 작은값의 비교를 쉽게 할 수 있다.
	3. 딥러닝등의 알고리즘은 Gradient Descent의 BAckpropagation개념에서 계산이 용이하게 편미분 된다.
	

### 손실에 대하여 (L2손실, L1손실 등)
	
L1손실은 모델이 예측하는 값과 라벨의 실제 값의 차이의 절대값에 기초한 손실 함수이다. 
	L1손실은 L2손실보다 이상점에 둔감하다.           
	L1= | y - prediction(x) | 선형회귀에 사용되는 손실 함수 이다. 
	L2함수는 라벨이 있는 예에 대한 모델의 예측 값과 라벨의 실제 값 간의 차이를 제곱하여 계산한다.         
	L2= ( y - prediction(x) )^2 
	L2 손실은 제곱 요소 때문에 부정확한 예측에 더 큰 벌점(PENALTY)를 준다. 따라서 이상점에 민감하다.

### L2 와 L1의 정규화. 그리고 이 둘의 비교
L1은 데이터의 갯수가 적을때 feature selection에 용이하게 사용되고, 
	L2는 데이터가 너무 많고, 결과값이 많은 feature의 영향을 받을때 사용하며 학습의 가중치를 완화시켜준다.          
	L1은 모델이 overfitting되는걸 방지하기 위해서 사용하는데, 
	이와 같은 경우에는 의미가 없다고 생각되는 feature에 가중치를 0.0으로 두게된다.                   
	그래서 feature자체가 학습이 되지 않게 된다.
	L2 같은 경우에는 가중치를 완화시켜 주기 떄문에 0으로 만들지는 않지만, 너무 모난 돌은 망치로 눌러버리는 효과가 있다.                   
	L2 정규화는 가중치를 작은 값으로 유도하지만 정확히 0.0으로 만들지는 못한다.             
	따라서 L1정규화를 사용하여 모델에서 유용하지 않은 많은 계수를 정확히 0이 되도록 유도하여 추론 단계에서 RAM을 절약할 수 있다.

###L1 정규화와 L2 정규화 비교

L2와 L1은 서로 다른 방식으로 가중치에 페널티를 준다.
	L2는 가중치2에 페널티를 준다.
	L1은 |가중치|에 페널티를 준다.                                  
        	이는 미분시 서로 다른 가중치를 갖게 되고 L1은 어떤 가중치를 정확히 0.0으로 만들 수 있다고 한다.
![](https://i.imgur.com/mBQgHUY.png)
![](https://i.imgur.com/y8XseH5.png)

1. L2에서 L1으로 정규화를 전환하면 테스트 손실과 학습 손실 사이의 멜타가 대폭 줄어든다.
2. L2에서 L1으로 정규화를 전환하면 학습된 모든 가중치를 완화한다.
3. L1 정규확률을 높으면 일반적으로 학습된 가중치가 완화되지만, 정규확률이 지나치게 높아지면 모델이 수렴할 수 없고 손실도 굉장히 높아진다.




### 손실을 줄이는 방법 - 경사하강법과 학습률에 대하여

경사하강법은 Cost Function을 가지고 천천히 파라미터들을 조정하면서 
	Cost Function의 최소값을 반복을 통해 찾아가도록 하는 알고리즘이다.   
	Cost Function의 데이터 결과를 그래프로 표시해보고 임의로 세타값을 정한 후 임의로 정한 파라미터를
	통해 Cost Function의 값을 정해지며 이제 여기서 주변의 부분을 최단거리로 찾아갈때 
	경사하강법 알고리즘이 작동하며 파라미터 초기화에 따라 최소값을 찾아가는 방법이 다를 수 있다.

그래프 표시

![](https://i.imgur.com/Tm5DyhE.png)

세타값 정함, Cost Function값 정해짐

![](https://i.imgur.com/udl7tMf.png)

최단거리로 찾아감

![](https://i.imgur.com/wHhzzXs.png)

경사하강법은 가능한 오차를 손실함수로 두고, 손실이 최소화가 되는 지점에 닿을 때까지 아래로 내려가며 모델을 학습시키는 방법이다. 손실이 최소화 된다는 것은 수학적으로 말하자면 기울기가 음(-)이 아닌 0값을 가질 때이다.

여기서 중요한 것은 ‘학습률’이다. 학습률은 아래로 내려갈 때 얼마나 넓은 보폭으로 내려갈 것인가를 의미한다. 학습률이 너무 작으면 조금씩밖에 움직일 수 없어 최적점에 도달하는데 오랜 시간이 걸린다. 반면 학습률이 너무 크면 최적점을 지나쳐서 오히려 상승, 폭발해버릴 수가 있다. 그래서 경사하강법을 사용할 때 이 학습률을 섬세하게 다룰 수 있어야 한다.

###   과적합이란?

   ![](https://i.imgur.com/HhFIYyA.png)

머신 러닝 (machine learning)에서 overfitting은 학습데이터를 과하게 잘 학습하는 것을 뜻한다. 일반적으로 학습 데이터는 실제 데이터의 부분집합인 경우가 대부분이다. 따라서, 아래의 그래프처럼 학습 데이터에 대해서는 오차가 감소하지만, 실제 데이터에 대해서는 오차가 증가하는 지점이 존재할 수 있다.


Overfitting을 이러한 관점에서 본다면 overfitting은 학습 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상이다. 예를 들어, 노란색 고양이를 보며 고양이의 특성을 학습한 사람이 검은색이나 흰색 고양이를 보고는 그것을 고양이라고 인식하지 못 하는 현상이 overfitting과 유사한 경우이다.

### 데이터셋을 분할하는 방법 (학습, 검증, 테스트)
학습 말그대로 데이터를 학습 시키는 것

검증 모델의 성능을 평가하기 위해서 사용함. 
학습 set의 일부를 모델의 성능을 평가하기 위해서 사용. 
	
테스트 test set은 모델의 최종 성능을 평가하기 위하여 쓰이고 training의 과정에 관여하지 않는다. 

 machine learning의 목적은 결국 unseen data 즉, test data에 대해 좋은 성능을 내는 것이다. 그러므로 모델을 만든 후 이 모델이 unseen data에 대해 얼마나 잘 동작할지에 대해서 반드시 확인이 필요하다. 하지만 training data를 사용해 성능을 평가하면 안되기 때문에 따로 validation set을 만들어 정확도를 측정하는 것이다. 두 번째는 모델을 튜닝하여 모델의 성능을 높일 수 있다. 예를 들어 overfitting 등을 막을 수 있다.
                       
예를 들어 training accuracy는 높은데 validation accuracy는 낮다면 데이터가 training set에 overfitting이 일어났을 가능성을 생각해볼 수 있다. 그렇다면 overfitting을 막아서 training accuracy를 희생하더라도 validation accuracy와 training accuracy를 비슷하게 맞춰줄 필요가 있다. 예를 들어 Deep learing을 모델을 구축한다면 regularization 과정을 한다거나 epoch을 줄이는 등의 방식으로 overfitting을 막을 수 있다. 
	
	
### 특성 벡터, 특성 추출이란
   ![](https://i.imgur.com/rRkqdMZ.png)
위 그림의 왼쪽 부분은 입력 데이터 소스의 원시 데이터이고 오른쪽 부분은 특성 벡터, 즉 데이터 세트의 예로 구성된 부동 소수점 값의 집합이다.                 
특성 추출이란 원시 데이터를 특성 벡터로 변환하는 과정이다.                           
 특성 추출에는 일반적으로 상당한 시간이 소요된다.
	
	
	여러 머신러닝 모델은 특성 값에 모델 가중치를 곱해야 하므로 실수 벡터로 가중치를 표현해야 한다.
	
### OOV(Out Of Vocabulary)

기계가 훈련 단계에서 학습한 단어들의 집합을 단어 집합(vocabulary)이라고 한다.                         
기계가 암기한 단어들의 리스트라고 생각할 수도 있겠다.                   
그리고 테스트 단계에서 기계가 미처 배우지 못한 모르는 단어가 등장한다면 이 단어들을 OOV(Out-Of-Vocabulary)라고 한다. 단어 집합에 없는 단어라는 의미이다. 이를 UNK(Unknown Word)로 표현하기도 한다. 결국 기계가 모르는 단어로 인해 문제를 풀지 못하는 상황을 OOV 문제라고 한다.

갑자기 용어들이 나와서 어려워보이지만 굉장히 단순한 이야기이다. 사람에 비유하면 통번역사를 꿈꾸는 사람이 한국어 문장들과 그에 대응되는 영어 문장들을 나름 공부했더라도, 배운 적 없는 생소한 한국어 단어에 대해서 통역을 해보라고하면 못 할 것이다.             
여기서 그 단어는 그 사람에게 있어 OOV에 해당된다. 모르는 단어이기때문에.


### 원 핫 인코딩, 멀티 핫 인코딩
원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다(1개만 True(Hot)이고 나머지는 Cold(False)). 이렇게 표현된 벡터를 원-핫 벡터(One-hot vector)라고 한다.
원-핫 인코딩을 두 가지 과정으로 정리해보면

(1) 각 단어에 고유한 인덱스를 부여한다. (정수 인코딩)

(2) 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여한다.

이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있다. 다른 말로는 벡터의 차원이 계속 늘어난다고도 표현한다. 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 된다. 
가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 된다. 다시 말해 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 되는데 이는 저장 공간 측면에서는 매우 비효율적인 표현 방법이다. 또한, 단어의 유사도를 표현하지 못한다는 단점이 있다.

멀티-핫 인코딩은 정수 시퀀스를 0과 1로 이루어진 벡터로 변환한다. 정확하게 말하면 시퀀스 [3, 5]를 인덱스 3과 5만 1이고 나머지는 모두 0인 10,000 차원 벡터로 변환한다는 의미이다.

### 선형 문제와 비선형 문제 

선형 문제는 직선 비례관계를 다루는 문제 뜻한다

비선형 문제는 선형 문제와 탄성문제와는 달리힘과 신장의 관계를 직선으로 나타낼 수 없는 문제를 말한다

### 시그모이드 함수, ReLU 함수의 특징과 적용
시그모이드 함수는 Logistic 함수라 불리기도한다. 선형인 멀티퍼셉트론에서 비선형 값을 얻기 위해 사용하기 시작했다.           

• 우선 함수값이 (0, 1)로 제한된다.

• 중간 값은 1212이다.

• 매우 큰 값을 가지면 함수값은 거의 1이며, 매우 작은 값을 가지면 거의 0이다.

![](https://i.imgur.com/sXXfdMM.png)


시그모이드 함수는 Gradient Vanishing현상이 발생한다. 미분함수에 대해 
x=0x=0에서 최대값 1414 을 가지고, input값이 일정이상 올라가면 미분값이 거의 0에 수렴하게된다. 이는 |x||x|값이 커질 수록 Gradient Backpropagation시 미분값이 소실될 가능성이 크다.
함수값 중심이 0이 아니다.
Exp 함수 사용시 비용이 크다.
따라서 최근에는 자주 사용하지 않게 되었다.
ReLu함수는 최근 가장많이 사용되는 활성화 함수이다.
   ![](https://i.imgur.com/C6Z9RLQ.png)


• x>0x>0 이면 기울기가 1인 직선이고, x<0x<0이면 함수값이 0이된다.

• sigmoid, tanh 함수와 비교시 학습이 훨씬 빨라진다.

• 연산 비용이 크지않고, 구현이 매우 간단하다.

• x<0x<0인 값들에 대해서는 기울기가 0이기 때문에 뉴런이 죽을 수 있는 단점이 존재한다.


### 로지스틱 회귀란? 로지스틱 회귀 모델에서의 정규화, 손실, 임계값
실제 많은 자연, 사회현상에서는 특정 변수에 대한 확률값이 선형이 아닌 S-커브 형태를 따르는 경우가 많다고 한다. 이러한 S-커브를 함수로 표현해낸 것이 바로 로지스틱 함수이다. 분야에 따라 시그모이드 함수로도 불리기도 한다.
로지스틱 회귀분석은 반응변수가 1 또는 0인 이진형 변수에서 쓰이는 회귀분석 방법이다.                                       
종속변수에 로짓변환을 실시하기 때문에 로지스틱 회귀분석이라고 불린다.                                              
로지스틱 회귀분석의 좋은점은 우선 계수가 Log Odds ratio가 되기 때문에 해석이 매우 편리하고, case-control과 같이 반응 변수에 따라 샘플링된 데이터에 대해서 편의(bias)가 없는 타당한 계수 추정치를 계산할 수 있다는 것이다.                                         
 이 부분에 초점을 맞추어 로지스틱 회귀분석에 대해 간단히 정리해보려고한다. 
정규화는 로지스틱 회귀 모델링에서 매우 중요하다. 정규화하지 않으면 로지스틱 회귀의 점근 특성이 고차원에서 계속 손실을 0으로 만들려고 시도하기 때문이다. 
결과적으로 대부분의 로지스틱 회귀 모델에서 모델 복잡성을 줄이기 위해 다음 두 전략 중 하나를 사용한다.

• L2 정규화

• 조기 중단, 즉 학습 단계 수 또는 학습률을 제한한다.                                             
로그시틱 회귀 모델에서의 손실 함수는 로그 손실(Log Loss)로 다음과 같이 정의 된다.

![](https://i.imgur.com/Opq29OS.png)


• (x,y) ∈ D: 라벨이 있는 예(x, y 쌍)가 많이 포함된 데이터 세트

• y: 라벨이 있는 예의 라벨(로지스틱 회귀이므로 y 값은 모두 0 또는 1)

• y': x의 특성 세트에 대한 예측 값(0 ~ 1 사이의 값)

로지스틱 회귀는 확률을 반환한다.                                        
반환된 확률을 '있는 그대로' 사용하거나(예: 사용자가 이 광고를 클릭할 확률은 0.00023임) 이진 값으로 변환하여(예: 이 이메일은 스팸임) 사용할 수 있다.
로지스틱 회귀 모형에서 특정 이메일에 관해 0.9995가 반환되면 이 이메일은 스팸일 가능성이 매우 높은 메일로 예측된 것이다.                               
이와 반대로 동일한 로지스틱 회귀 모형에서 예측 점수가 0.0003점인 다른 이메일은 스팸이 아닐 가능성이 높다.                             
그렇다면 예측 점수가 0.6점인 이메일은 어떨까? 
로지스틱 회귀 값을 이진 카테고리에 매핑하려면 분류 임계값(결정 임계값이라고도 함)을 정의해야 한다. 임계값보다 높은 값은 '스팸'을 나타내고 임계값보다 낮은 값은 '스팸 아님'을 나타낸다. 분류 임계값은 항상 0.5여야 한다고 생각하기 쉽지만 임계값은 문제에 따라 달라지므로 값을 조정해야 한다.
# NN
## 신경망이란?
인간이 뇌를 통해 문제를 처리하는 방법과 비슷한 방법으로 문제를 해결하기 위해 컴퓨터에서 채택하고 있는 구조.               
인간은 뇌의 기본 구조 조직인 뉴런(neuron)과 뉴런이 연결되어 일을 처리하는 것처럼, 수학적 모델로서의 뉴런이 상호 연결되어 네트워크를 형성할 때 이를 신경망이라 한다.

## NN의 구조 (어떻게 구성되는가?)
![](https://i.imgur.com/65iszre.jpg)
신경망(Neural Networks)은 위의 그림처럼 여러가자의 뉴런이 각 층(layer)을 이루면서 서로 연결 되어 있는 구조를 가진다.                       
완전히 다 연결된 층(fully-connected layer)은 가장 흔한 신경망의 층 구조인데, 이는 이전 층의 한 뉴런은 다음층의 모든 뉴런과 연결되어 있는 구조를 말한다.                       
     위의 경우가 완전히 다 연결된 층의 예시이다.


## 전향 전파법 (forward propagation)
 뉴럴 네트워크 모델의 입력층부터 출력층까지 순서대로 변수들을 계산하고 저장하는 것을 의미한다.
연산 그래프를 도식화하면 연산에 포함된 연산자와 변수들 사이의 관계를 시각화 하는데 도움이 된다 .                 


## 시그모이드 함수를 어디에 적용하는가?
시그모이드 함수는 직선함수를 곡선 함수로 변형하기 위해서사용한다. 
시그모이드 함수를 지수함수에 넣음으로서 그래프가 굴곡지게 된다.

## 역전파법 (Backpropagation)
역전파(back propagation)는 뉴럴 네트워크의 파라미터들에 대한 그래디언트(gradient)를 계산하는 방법을 의미한다.                  
일반적으로는 역전파(back propagation)은 뉴럴 네트워크의 각 층과 관련된 목적 함수(objective function)의 중간 변수들과 파라미터들의 그래디언트(gradient)를 출력층에서 입력층 순으로 계산하고 저장한다.                 
이는 미적분의 ’체인룰(chain rule)’을 따르기 때문이다.

• 전향전파(forwards propagation)는 뉴럴 네트워크의 그래프를 계산하기 위해서 중간 변수들을 순서대로 계산하고 저장합니다. 즉, 입력층부터 시작해서 출력층까지 처리한다.

• 역전파(back propagation)은 중간 변수와 파라미터에 대한 그래디언트(gradient)를 반대 방향으로 계산하고 저장한다.


